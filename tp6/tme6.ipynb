{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Loading datasets...\n",
      "INFO:root:Vocabulary size: 42930\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "\n",
    "from datamaestro import prepare_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import time\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "ds = prepare_dataset('org.universaldependencies.french.gsd')\n",
    "\n",
    "\n",
    "# Format de sortie décrit dans\n",
    "# https://pypi.org/project/conllu/\n",
    "\n",
    "class Vocabulary:\n",
    "    \"\"\"Permet de gérer un vocabulaire.\n",
    "\n",
    "    En test, il est possible qu'un mot ne soit pas dans le\n",
    "    vocabulaire : dans ce cas le token \"__OOV__\" est utilisé.\n",
    "    Attention : il faut tenir compte de cela lors de l'apprentissage !\n",
    "\n",
    "    Utilisation:\n",
    "\n",
    "    - en train, utiliser v.get(\"blah\", adding=True) pour que le mot soit ajouté\n",
    "      automatiquement\n",
    "    - en test, utiliser v[\"blah\"] pour récupérer l'ID du mot (ou l'ID de OOV)\n",
    "    \"\"\"\n",
    "    OOVID = 1\n",
    "    PAD = 0\n",
    "\n",
    "    def __init__(self, oov: bool):\n",
    "        self.oov =  oov\n",
    "        self.id2word = [ \"PAD\"]\n",
    "        self.word2id = { \"PAD\" : Vocabulary.PAD}\n",
    "        if oov:\n",
    "            self.word2id[\"__OOV__\"] = Vocabulary.OOVID\n",
    "            self.id2word.append(\"__OOV__\")\n",
    "\n",
    "    def __getitem__(self, word: str):\n",
    "        if self.oov:\n",
    "            return self.word2id.get(word, Vocabulary.OOVID)\n",
    "        return self.word2id[word]\n",
    "\n",
    "    def get(self, word: str, adding=True):\n",
    "        try:\n",
    "            return self.word2id[word]\n",
    "        except KeyError:\n",
    "            if adding:\n",
    "                wordid = len(self.id2word)\n",
    "                self.word2id[word] = wordid\n",
    "                self.id2word.append(word)\n",
    "                return wordid\n",
    "            if self.oov:\n",
    "                return Vocabulary.OOVID\n",
    "            raise\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.id2word)\n",
    "\n",
    "    def getword(self,idx: int):\n",
    "        if idx < len(self):\n",
    "            return self.id2word[idx]\n",
    "        return None\n",
    "\n",
    "    def getwords(self,idx: List[int]):\n",
    "        return [self.getword(i) for i in idx]\n",
    "\n",
    "\n",
    "\n",
    "class TaggingDataset():\n",
    "    def __init__(self, data, words: Vocabulary, tags: Vocabulary, adding=True):\n",
    "        self.sentences = []\n",
    "\n",
    "        for s in data:\n",
    "            self.sentences.append(([words.get(token[\"form\"], adding) for token in s], [tags.get(token[\"upostag\"], adding) for token in s]))\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    def __getitem__(self, ix):\n",
    "        return self.sentences[ix]\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    \"\"\"Collate using pad_sequence\"\"\"\n",
    "    return tuple(pad_sequence([torch.LongTensor(b[j]) for b in batch]) for j in range(2))\n",
    "\n",
    "\n",
    "logging.info(\"Loading datasets...\")\n",
    "words = Vocabulary(True)\n",
    "tags = Vocabulary(False)\n",
    "train_data = TaggingDataset(ds.train, words, tags, True)\n",
    "dev_data = TaggingDataset(ds.validation, words, tags, True)\n",
    "test_data = TaggingDataset(ds.test, words, tags, False)\n",
    "\n",
    "\n",
    "logging.info(\"Vocabulary size: %d\", len(words))\n",
    "\n",
    "\n",
    "BATCH_SIZE=100\n",
    "\n",
    "train_loader = DataLoader(train_data, collate_fn=collate, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_data, collate_fn=collate, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_data, collate_fn=collate, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTag(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, target_size):\n",
    "        super(LSTMTag, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, target_size)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, (hidden_state, cell_state) = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out)\n",
    "        return tag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "metadata": {},
     "execution_count": 128
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "epoch 0 : Loss 2.9518022537231445\n",
      "epoch 0 : Loss 2.8045825958251953\n",
      "epoch 0 : Loss 3.4375267028808594\n",
      "epoch 0 : Loss 2.165649175643921\n",
      "epoch 0 : Loss 1.6507859230041504\n",
      "epoch 0 : Loss 1.412873387336731\n",
      "epoch 0 : Loss 1.191569447517395\n",
      "epoch 0 : Loss 0.9852752685546875\n",
      "epoch 0 : Loss 0.9779251217842102\n",
      "epoch 0 : Loss 0.8854538202285767\n",
      "epoch 0 : Loss 0.9006351232528687\n",
      "epoch 0 : Loss 0.834964394569397\n",
      "epoch 0 : Loss 0.7934458255767822\n",
      "epoch 0 : Loss 0.7474129796028137\n",
      "epoch 0 : Loss 0.7518255710601807\n",
      "epoch 0 : Loss 0.7360935807228088\n",
      "epoch 0 : Loss 0.6953868269920349\n",
      "epoch 0 : Loss 0.6793655753135681\n",
      "epoch 0 : Loss 0.6263436675071716\n",
      "epoch 0 : Loss 0.6632508039474487\n",
      "epoch 0 : Loss 0.6042245626449585\n",
      "epoch 0 : Loss 0.5866310596466064\n",
      "epoch 0 : Loss 0.5810867547988892\n",
      "epoch 0 : Loss 0.5633209347724915\n",
      "epoch 0 : Loss 0.5712041258811951\n",
      "epoch 0 : Loss 0.6019901633262634\n",
      "epoch 0 : Loss 0.5561022162437439\n",
      "epoch 0 : Loss 0.5132210850715637\n",
      "epoch 0 : Loss 0.47939828038215637\n",
      "epoch 0 : Loss 0.5244255065917969\n",
      "epoch 0 : Loss 0.5221205353736877\n",
      "epoch 0 : Loss 0.5572004914283752\n",
      "epoch 0 : Loss 0.47333967685699463\n",
      "epoch 0 : Loss 0.4875611960887909\n",
      "epoch 0 : Loss 0.4766712784767151\n",
      "epoch 0 : Loss 0.4412331283092499\n",
      "epoch 0 : Loss 0.4951944649219513\n",
      "epoch 0 : Loss 0.4341675341129303\n",
      "epoch 0 : Loss 0.4206499457359314\n",
      "epoch 0 : Loss 0.47441232204437256\n",
      "epoch 0 : Loss 0.40800634026527405\n",
      "epoch 0 : Loss 0.43244239687919617\n",
      "epoch 0 : Loss 0.4220130741596222\n",
      "epoch 0 : Loss 0.4297337234020233\n",
      "epoch 0 : Loss 0.39659276604652405\n",
      "epoch 0 : Loss 0.3925643563270569\n",
      "epoch 0 : Loss 0.39064109325408936\n",
      "epoch 0 : Loss 0.3557245433330536\n",
      "epoch 0 : Loss 0.42532140016555786\n",
      "epoch 0 : Loss 0.3779889643192291\n",
      "epoch 0 : Loss 0.4052441418170929\n",
      "epoch 0 : Loss 0.42046698927879333\n",
      "epoch 0 : Loss 0.39752423763275146\n",
      "epoch 0 : Loss 0.4094494581222534\n",
      "epoch 0 : Loss 0.40463173389434814\n",
      "epoch 0 : Loss 0.37890276312828064\n",
      "epoch 0 : Loss 0.4405767619609833\n",
      "epoch 0 : Loss 0.3711031675338745\n",
      "epoch 0 : Loss 0.36593854427337646\n",
      "epoch 0 : Loss 0.3761700689792633\n",
      "epoch 0 : Loss 0.34882616996765137\n",
      "epoch 0 : Loss 0.3770865201950073\n",
      "epoch 0 : Loss 0.32522958517074585\n",
      "epoch 0 : Loss 0.3568001687526703\n",
      "epoch 0 : Loss 0.3344508409500122\n",
      "epoch 0 : Loss 0.37100136280059814\n",
      "epoch 0 : Loss 0.4008376896381378\n",
      "epoch 0 : Loss 0.40734410285949707\n",
      "epoch 0 : Loss 0.37983715534210205\n",
      "epoch 0 : Loss 0.31569960713386536\n",
      "epoch 0 : Loss 0.3534287214279175\n",
      "epoch 0 : Loss 0.3313518166542053\n",
      "epoch 0 : Loss 0.3594563603401184\n",
      "epoch 0 : Loss 0.35380956530570984\n",
      "epoch 0 : Loss 0.3795970380306244\n",
      "epoch 0 : Loss 0.3387869596481323\n",
      "epoch 0 : Loss 0.31496724486351013\n",
      "epoch 0 : Loss 0.33876535296440125\n",
      "epoch 0 : Loss 0.34102141857147217\n",
      "epoch 0 : Loss 0.3105206787586212\n",
      "epoch 0 : Loss 0.33155155181884766\n",
      "epoch 0 : Loss 0.36733463406562805\n",
      "epoch 0 : Loss 0.3107646703720093\n",
      "epoch 0 : Loss 0.3651242256164551\n",
      "epoch 0 : Loss 0.2910451591014862\n",
      "epoch 0 : Loss 0.31674519181251526\n",
      "epoch 0 : Loss 0.32703322172164917\n",
      "epoch 0 : Loss 0.29505831003189087\n",
      "epoch 0 : Loss 0.32094287872314453\n",
      "epoch 0 : Loss 0.33342036604881287\n",
      "epoch 0 : Loss 0.29813387989997864\n",
      "epoch 0 : Loss 0.2984602749347687\n",
      "epoch 0 : Loss 0.3415543735027313\n",
      "epoch 0 : Loss 0.31373679637908936\n",
      "epoch 0 : Loss 0.3101346790790558\n",
      "epoch 0 : Loss 0.36762484908103943\n",
      "epoch 0 : Loss 0.26882442831993103\n",
      "epoch 0 : Loss 0.31292417645454407\n",
      "epoch 0 : Loss 0.29203999042510986\n",
      "epoch 0 : Loss 0.2971929907798767\n",
      "epoch 0 : Loss 0.30657172203063965\n",
      "epoch 0 : Loss 0.3030298352241516\n",
      "epoch 0 : Loss 0.28726744651794434\n",
      "epoch 0 : Loss 0.3275359272956848\n",
      "epoch 0 : Loss 0.29707592725753784\n",
      "epoch 0 : Loss 0.2737084925174713\n",
      "epoch 0 : Loss 0.29174619913101196\n",
      "epoch 0 : Loss 0.3536486327648163\n",
      "epoch 0 : Loss 0.30472445487976074\n",
      "epoch 0 : Loss 0.2931392192840576\n",
      "epoch 0 : Loss 0.27849557995796204\n",
      "epoch 0 : Loss 0.3220153748989105\n",
      "epoch 0 : Loss 0.34660977125167847\n",
      "epoch 0 : Loss 0.2928157150745392\n",
      "epoch 0 : Loss 0.2899315059185028\n",
      "epoch 0 : Loss 0.29064497351646423\n",
      "epoch 0 : Loss 0.29641610383987427\n",
      "epoch 0 : Loss 0.290882408618927\n",
      "epoch 0 : Loss 0.3022100031375885\n",
      "epoch 0 : Loss 0.3114110827445984\n",
      "epoch 0 : Loss 0.27260100841522217\n",
      "epoch 0 : Loss 0.27583298087120056\n",
      "epoch 0 : Loss 0.30903610587120056\n",
      "epoch 0 : Loss 0.2637961208820343\n",
      "epoch 0 : Loss 0.2707175016403198\n",
      "epoch 0 : Loss 0.30643904209136963\n",
      "epoch 0 : Loss 0.3096679747104645\n",
      "epoch 0 : Loss 0.2892235517501831\n",
      "epoch 0 : Loss 0.2976287603378296\n",
      "epoch 0 : Loss 0.24797959625720978\n",
      "epoch 0 : Loss 0.2819954454898834\n",
      "epoch 0 : Loss 0.28562742471694946\n",
      "epoch 0 : Loss 0.27599045634269714\n",
      "epoch 0 : Loss 0.3346937894821167\n",
      "epoch 0 : Loss 0.2670968770980835\n",
      "epoch 0 : Loss 0.294589102268219\n",
      "epoch 0 : Loss 0.30015119910240173\n",
      "epoch 0 : Loss 0.2633875608444214\n",
      "epoch 0 : Loss 0.257465660572052\n",
      "epoch 0 : Loss 0.27623024582862854\n",
      "epoch 0 : Loss 0.2953007221221924\n",
      "epoch 0 : Loss 0.33077946305274963\n",
      "epoch 0 : Loss 0.3149436116218567\n",
      "epoch 0 : Loss 0.2768327295780182\n",
      "epoch 0 : Loss 0.2869552671909332\n",
      "epoch 1 : Loss 0.20466598868370056\n",
      "epoch 1 : Loss 0.19011016190052032\n",
      "epoch 1 : Loss 0.18973729014396667\n",
      "epoch 1 : Loss 0.21244268119335175\n",
      "epoch 1 : Loss 0.19702160358428955\n",
      "epoch 1 : Loss 0.17627349495887756\n",
      "epoch 1 : Loss 0.2013123333454132\n",
      "epoch 1 : Loss 0.19147011637687683\n",
      "epoch 1 : Loss 0.1942438781261444\n",
      "epoch 1 : Loss 0.21016578376293182\n",
      "epoch 1 : Loss 0.2196386605501175\n",
      "epoch 1 : Loss 0.18656271696090698\n",
      "epoch 1 : Loss 0.17350587248802185\n",
      "epoch 1 : Loss 0.20519983768463135\n",
      "epoch 1 : Loss 0.16999641060829163\n",
      "epoch 1 : Loss 0.21519406139850616\n",
      "epoch 1 : Loss 0.2285728007555008\n",
      "epoch 1 : Loss 0.20937560498714447\n",
      "epoch 1 : Loss 0.18291449546813965\n",
      "epoch 1 : Loss 0.17418208718299866\n",
      "epoch 1 : Loss 0.21493349969387054\n",
      "epoch 1 : Loss 0.2030133306980133\n",
      "epoch 1 : Loss 0.2457694709300995\n",
      "epoch 1 : Loss 0.20347793400287628\n",
      "epoch 1 : Loss 0.19104374945163727\n",
      "epoch 1 : Loss 0.21896153688430786\n",
      "epoch 1 : Loss 0.18255077302455902\n",
      "epoch 1 : Loss 0.1818702518939972\n",
      "epoch 1 : Loss 0.20021410286426544\n",
      "epoch 1 : Loss 0.23462291061878204\n",
      "epoch 1 : Loss 0.18712040781974792\n",
      "epoch 1 : Loss 0.21664564311504364\n",
      "epoch 1 : Loss 0.20747612416744232\n",
      "epoch 1 : Loss 0.173552468419075\n",
      "epoch 1 : Loss 0.2019592672586441\n",
      "epoch 1 : Loss 0.17867882549762726\n",
      "epoch 1 : Loss 0.20223675668239594\n",
      "epoch 1 : Loss 0.18182893097400665\n",
      "epoch 1 : Loss 0.23471753299236298\n",
      "epoch 1 : Loss 0.20343901216983795\n",
      "epoch 1 : Loss 0.21557193994522095\n",
      "epoch 1 : Loss 0.19997820258140564\n",
      "epoch 1 : Loss 0.18388645350933075\n",
      "epoch 1 : Loss 0.2298305481672287\n",
      "epoch 1 : Loss 0.19683125615119934\n",
      "epoch 1 : Loss 0.2092968225479126\n",
      "epoch 1 : Loss 0.2220432311296463\n",
      "epoch 1 : Loss 0.22485807538032532\n",
      "epoch 1 : Loss 0.18188191950321198\n",
      "epoch 1 : Loss 0.19492961466312408\n",
      "epoch 1 : Loss 0.17715632915496826\n",
      "epoch 1 : Loss 0.19013866782188416\n",
      "epoch 1 : Loss 0.19474998116493225\n",
      "epoch 1 : Loss 0.24072852730751038\n",
      "epoch 1 : Loss 0.21185384690761566\n",
      "epoch 1 : Loss 0.24048008024692535\n",
      "epoch 1 : Loss 0.21214796602725983\n",
      "epoch 1 : Loss 0.21473398804664612\n",
      "epoch 1 : Loss 0.24953383207321167\n",
      "epoch 1 : Loss 0.17646516859531403\n",
      "epoch 1 : Loss 0.1973920464515686\n",
      "epoch 1 : Loss 0.25460103154182434\n",
      "epoch 1 : Loss 0.22331741452217102\n",
      "epoch 1 : Loss 0.20476552844047546\n",
      "epoch 1 : Loss 0.1963774859905243\n",
      "epoch 1 : Loss 0.2001788467168808\n",
      "epoch 1 : Loss 0.21180152893066406\n",
      "epoch 1 : Loss 0.1718086153268814\n",
      "epoch 1 : Loss 0.19546861946582794\n",
      "epoch 1 : Loss 0.21029965579509735\n",
      "epoch 1 : Loss 0.2565576434135437\n",
      "epoch 1 : Loss 0.22659432888031006\n",
      "epoch 1 : Loss 0.23150041699409485\n",
      "epoch 1 : Loss 0.2048407942056656\n",
      "epoch 1 : Loss 0.1882156878709793\n",
      "epoch 1 : Loss 0.19983820617198944\n",
      "epoch 1 : Loss 0.2210574597120285\n",
      "epoch 1 : Loss 0.1997358798980713\n",
      "epoch 1 : Loss 0.1955558955669403\n",
      "epoch 1 : Loss 0.21811135113239288\n",
      "epoch 1 : Loss 0.22506245970726013\n",
      "epoch 1 : Loss 0.1887834668159485\n",
      "epoch 1 : Loss 0.22076964378356934\n",
      "epoch 1 : Loss 0.19050829112529755\n",
      "epoch 1 : Loss 0.21116849780082703\n",
      "epoch 1 : Loss 0.17225925624370575\n",
      "epoch 1 : Loss 0.19605371356010437\n",
      "epoch 1 : Loss 0.21706265211105347\n",
      "epoch 1 : Loss 0.20615074038505554\n",
      "epoch 1 : Loss 0.20122526586055756\n",
      "epoch 1 : Loss 0.17166219651699066\n",
      "epoch 1 : Loss 0.2285802662372589\n",
      "epoch 1 : Loss 0.2073025107383728\n",
      "epoch 1 : Loss 0.20334701240062714\n",
      "epoch 1 : Loss 0.21182891726493835\n",
      "epoch 1 : Loss 0.1901654452085495\n",
      "epoch 1 : Loss 0.20736286044120789\n",
      "epoch 1 : Loss 0.1949712187051773\n",
      "epoch 1 : Loss 0.16014544665813446\n",
      "epoch 1 : Loss 0.1752229481935501\n",
      "epoch 1 : Loss 0.1875908374786377\n",
      "epoch 1 : Loss 0.25738993287086487\n",
      "epoch 1 : Loss 0.21941113471984863\n",
      "epoch 1 : Loss 0.19041207432746887\n",
      "epoch 1 : Loss 0.17898839712142944\n",
      "epoch 1 : Loss 0.21564383804798126\n",
      "epoch 1 : Loss 0.18490095436573029\n",
      "epoch 1 : Loss 0.19131723046302795\n",
      "epoch 1 : Loss 0.20075908303260803\n",
      "epoch 1 : Loss 0.1861579418182373\n",
      "epoch 1 : Loss 0.2043127715587616\n",
      "epoch 1 : Loss 0.1929919719696045\n",
      "epoch 1 : Loss 0.1780897080898285\n",
      "epoch 1 : Loss 0.19723166525363922\n",
      "epoch 1 : Loss 0.20744837820529938\n",
      "epoch 1 : Loss 0.18239955604076385\n",
      "epoch 1 : Loss 0.21987438201904297\n",
      "epoch 1 : Loss 0.20248953998088837\n",
      "epoch 1 : Loss 0.19256410002708435\n",
      "epoch 1 : Loss 0.21882328391075134\n",
      "epoch 1 : Loss 0.20026467740535736\n",
      "epoch 1 : Loss 0.19759716093540192\n",
      "epoch 1 : Loss 0.16837334632873535\n",
      "epoch 1 : Loss 0.19350512325763702\n",
      "epoch 1 : Loss 0.1757434606552124\n",
      "epoch 1 : Loss 0.20724201202392578\n",
      "epoch 1 : Loss 0.19323895871639252\n",
      "epoch 1 : Loss 0.20851075649261475\n",
      "epoch 1 : Loss 0.21965444087982178\n",
      "epoch 1 : Loss 0.19592133164405823\n",
      "epoch 1 : Loss 0.2288457155227661\n",
      "epoch 1 : Loss 0.17789801955223083\n",
      "epoch 1 : Loss 0.18258188664913177\n",
      "epoch 1 : Loss 0.1825389862060547\n",
      "epoch 1 : Loss 0.18858322501182556\n",
      "epoch 1 : Loss 0.20037679374217987\n",
      "epoch 1 : Loss 0.20143190026283264\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-c97a37721370>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;31m#import ipdb;ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-52c02417eaa4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtag_space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python3_env/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    570\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 60\n",
    "HIDDEN_DIM = 120\n",
    "\n",
    "\n",
    "model = LSTMTag(EMBEDDING_DIM, HIDDEN_DIM, len(words), len(tags))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(50):\n",
    "    for sentence, tag in train_loader:\n",
    "        target = tag.permute(1,0)\n",
    "        model.zero_grad()\n",
    "        tag_scores = model(sentence).permute(1,2,0)\n",
    "        #import ipdb;ipdb.set_trace()\n",
    "        loss = criterion(tag_scores, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch {epoch} : Loss {loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Finished!!!\\nAgain testing on unknown data\")\n",
    "with torch.no_grad():\n",
    "    for seq in [seq1, seq2]:\n",
    "        inputs = prepare_sequence(seq, word_to_ix)\n",
    "        tag_scores = model(inputs)\n",
    "        _, indices = torch.max(tag_scores, 1)\n",
    "        ret = []\n",
    "        for i in range(len(indices)):\n",
    "            for key, value in tag_to_ix.items():\n",
    "                if indices[i] == value:\n",
    "                    ret.append((seq[i], key))\n",
    "        print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "([21, 22, 23, 24, 9, 18, 25, 26, 27, 14, 28, 29, 9, 30, 31, 27, 32, 20],\n",
       " [1, 2, 4, 5, 7, 1, 2, 11, 7, 1, 2, 10, 7, 1, 2, 7, 12, 10])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "train_data.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'œuvre'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "tags.getword(train_data.__getitem__(0)[1][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, tag in train_loader:\n",
    "    a = tag\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([58, 19])"
      ]
     },
     "metadata": {},
     "execution_count": 117
    }
   ],
   "source": [
    "F.one_hot(a[:,0],19).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 121
    }
   ],
   "source": [
    "F.one_hot(a,19)[:,0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "F.one_hot(a[:,0],19)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  9,  5,  6,  7,  1,  2, 10,  1,  2, 11,  7,  1,  2, 16, 16, 10,\n",
       "        12, 12, 13, 12, 12, 12, 10,  5,  1,  2, 10,  5,  7,  2,  7, 12, 12, 10,\n",
       "         2,  7, 12, 12, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0])"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "a[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bit4e7958304e6340d8b3815d40265de01f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}