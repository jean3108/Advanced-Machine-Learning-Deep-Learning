{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1) Pour l'entrainement du RNN pour la génération (tp4), on utilise des séquences fixes. Qu'est-ce que votre dataset vous renvoie comme X et y? \n",
    "Car de mon coté, mon dataset va par exemple me donner X=\"thank yo\" et y=\"u\". Mais de doit-on pas plutot renvoyer X=\"thank you\" et pas de y puis entrainer le modèle en prenant:\n",
    "- X=\"t\" et y=\"h\" puis \n",
    "- X=\"th\" et y=\"a\" puis\n",
    "- X=\"tha\" et y=\"n\" etc ?\n",
    "\n",
    "on va par exemple me donner la séquence \"thank yo\" et la taget associé sera \"u\". C'est ce que vous faites? (pcq je crois qu'on peut pour une séquence \"thank you\" , on peut faire autant d'entrainement que de longueur de séquence genre on prend \"t\" pour prédire \"h\" puis \"th\" pour prédire \"a\" puis \"tha\" pour prédire \"n\" etc, jusqu'au botu de la séquence?)\n",
    "\n",
    "Vu le code du tp5, le loader peut ne renvoyer que un X (et pas un couple X,y) donc peut etre qu'il faut re,voyer une sequence et faire l'apprentissage sur toutes les sous-sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fill_na(mat):\n",
    "    ix,iy = np.where(np.isnan(mat))\n",
    "    for i,j in zip(ix,iy):\n",
    "        if np.isnan(mat[i+1,j]):\n",
    "            mat[i,j]=mat[i-1,j]\n",
    "        else:\n",
    "            mat[i,j]=(mat[i-1,j]+mat[i+1,j])/2.\n",
    "    return mat\n",
    "\n",
    "\n",
    "def read_temps(path):\n",
    "    \"\"\"Lit le fichier de températures\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"rt\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if not row[1].replace(\".\",\"\").isdigit():\n",
    "                continue\n",
    "            data.append([float(x) if x != \"\" else float('nan') for x in row[1:]])\n",
    "    return torch.tensor(fill_na(np.array(data)), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNv1(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_encode = torch.tanh\n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearX = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.linearH = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.act_encode(self.linearX(x) + self.linearH(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.RNNcell = nn.RNNCell(input_size=input_dim, hidden_size=latent_dim, bias=True, nonlinearity='tanh')\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.RNNcell(x, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUv1(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearZ = nn.Linear(input_dim+latent_dim, latent_dim, bias=False)\n",
    "        self.linearR = nn.Linear(input_dim+latent_dim, input_dim+latent_dim, bias=False)\n",
    "        self.linearH = nn.Linear(input_dim+latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        zt = self.sigmoid(self.linearZ(concatHX))\n",
    "        rt = self.sigmoid(self.linearR(concatHX))\n",
    "        ht = (1-zt)*h + zt* self.tanh(self.linearH(rt*concatHX))\n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.GRUcell = nn.GRUCell(input_size=input_dim, hidden_size=latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.GRUcell(x, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMv1(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "        \n",
    "        self.ct = torch.zeros((BATCH_SIZE, latent_dim))\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearF = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearI = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearC = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearO = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        ft = self.sigmoid(self.linearF(concatHX))\n",
    "        it = self.sigmoid(self.linearI(concatHX))\n",
    "        newCt = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        #self.ct = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        ot = self.sigmoid(self.linearO(concatHX))\n",
    "        ht = ot*self.tanh(newCt)\n",
    "        self.ct = newCt\n",
    "        \n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMv2(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "        \n",
    "        self.cts = [torch.zeros((BATCH_SIZE, latent_dim))]\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearF = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearI = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearC = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearO = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        ft = self.sigmoid(self.linearF(concatHX))\n",
    "        it = self.sigmoid(self.linearI(concatHX))\n",
    "        ct = ft*self.cts[-1] + it*self.tanh(self.linearC(concatHX))\n",
    "        #self.ct = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        ot = self.sigmoid(self.linearO(concatHX))\n",
    "        ht = ot*self.tanh(ct)\n",
    "        \n",
    "        self.cts.append(ct)\n",
    "        \n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        #delete all cts\n",
    "        #self.cts = [self.cts[-1]]\n",
    "        \n",
    "        #forward\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.tanh = torch.tanh\n",
    "        \n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.LSTMcell = nnLSTMCell(input_size=input_dim, hidden_size=latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.LSTMcell(x, h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        #delete all cts\n",
    "        #self.cts = [self.cts[-1]]\n",
    "        \n",
    "        #forward\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_temp(Dataset):\n",
    "    def __init__(self, data, target, lenght=50):\n",
    "        self.data = data\n",
    "        self.lenght = lenght\n",
    "        self.size = self.data.shape[0]-self.lenght+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col = index//self.size\n",
    "        lin = index%self.size\n",
    "        return (self.data[lin:lin+self.lenght, col], col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size*self.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(2)\n",
    "temp_test = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasse = 5\n",
    "longueurData = 30\n",
    "BATCH_SIZE = 6\n",
    "longueurSeq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = temp_train[:longueurData, :nbClasse]\n",
    "temp_test = temp_test[:longueurData, :nbClasse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_temp(temp_train, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "latent_size = 10\n",
    "input_dim = 1\n",
    "output_dim = nbClasse\n",
    "lr=1e-3\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Itérations 0: train loss 1.9284852743148804, test loss 1.5937219858169556\n",
      "Itérations 1: train loss 1.7714735269546509, test loss 1.7347369194030762\n",
      "Itérations 2: train loss 1.7616195678710938, test loss 1.414864420890808\n",
      "Itérations 3: train loss 1.8163965940475464, test loss 1.6045713424682617\n",
      "Itérations 4: train loss 1.6932960748672485, test loss 1.6150871515274048\n",
      "Itérations 5: train loss 1.6077475547790527, test loss 1.4935122728347778\n",
      "Itérations 6: train loss 1.5200608968734741, test loss 1.5941352844238281\n",
      "Itérations 7: train loss 1.7681688070297241, test loss 1.613354206085205\n",
      "Itérations 8: train loss 1.571211338043213, test loss 1.8035459518432617\n",
      "Itérations 9: train loss 1.6167024374008179, test loss 1.371878743171692\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_states = model(sequences.permute(1,0,2))\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            hidden_states = model(sequences.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            test_loss = criterion(outputs, labels)\n",
    "\n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LETTRES = string.ascii_letters + string.punctuation+string.digits+' '\n",
    "#LETTRES = string.ascii_letters+' '\n",
    "LETTRES = string.ascii_letters[:26]+\".\"+' '\n",
    "id2lettre = dict(zip(range(1, len(LETTRES)+1), LETTRES))\n",
    "id2lettre[0] = ''\n",
    "lettre2id = dict(zip(id2lettre.values(), id2lettre.keys()))\n",
    "\n",
    "def normalize(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if c in LETTRES)\n",
    "\n",
    "def string2code(s):\n",
    "    return torch.tensor([lettre2id[c] for c in normalize(s)])\n",
    "\n",
    "def code2string(t):\n",
    "    if(type(t)!=list):\n",
    "        t = t.tolist()\n",
    "    return ''.join(id2lettre[i] for i in t)\n",
    "\n",
    "def str2code(s):\n",
    "    return [lettre2id[c] for c in s]\n",
    "\n",
    "def strs2code(ss):\n",
    "    return torch.LongTensor([str2code(s) for s in ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-traitement données trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTrumpData(s):\n",
    "    tmp = re.sub(\"\\[[^]]+\\]\", \"\", s) #delete non vocan words as [applause]\n",
    "    tmp = re.sub(\"[.?!]\", \".\", tmp)#replace end of phrase by .\n",
    "    tmp = re.sub(\":\\s*pmurT\\s*\\.\", \":%.\", tmp[::-1]) #reverse string and replace trump by %\n",
    "    tmp = re.sub(\":[^.%]+?\\.\", \":@.\", tmp) # place all no trump speaker by @\n",
    "    tmp = re.sub(\"^\\s*Trump\", \"%\", tmp[::-1]) #reverse string and replace first Trump by %\n",
    "    tmp = re.sub(\"@\\s*:[^%]+?%\", \"%\", tmp)  #delete words not say by trump\n",
    "    return re.sub(\"%:\", \"\", tmp)# delete %: wich is just to show wo speaks (but now it is trump every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/trump_full_speech.txt\", 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanedData = cleanTrumpData(data)\n",
    "cleanedData = cleanTrumpData(data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" wow. whoa. that is some group of people. thousands. so nice, thank you very much. that's really nice. thank you. it's great to be at trump tower. it's great to be in a wonderful city, new york. and it's an honor to have everybody here. this is beyond anybody's expectations. there's been no crowd like this. and, i can tell, some of the candidates, they went in. they didn't know the air-conditioner didn't work. they sweated like dogs.  they didn't know the room was too big, because they didn't have anybody there. how are they going to beat isis. i don't think it's gonna happen.  our country is in serious trouble. we don't have victories anymore. we used to have victories, but we don't have them. when was the last time anybody saw us beating, let's say, china in a trade deal. they kill us. i beat china all the time. all the time. when did we beat japan at anything. they send their cars over by the millions, and what do we do. when was the last time you saw a chevrolet in tokyo. it doesn'\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedData[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedNormalizedData = normalize(cleanedData)\n",
    "cleanedNormalizedData = cleanedNormalizedData[:10000]#To have a little sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wow. whoa. that is some group of people. thousands. so nice thank you very much. thats really nice. thank you. its great to be at trump tower. its great to be in a wonderful city new york. and its an honor to have everybody here. this is beyond anybodys expectations. theres been no crowd like this. and i can tell some of the candidates they went in. they didnt know the airconditioner didnt work. they sweated like dogs.  they didnt know the room was too big because they didnt have anybody there. how are they going to beat isis. i dont think its gonna happen.  our country is in serious trouble. we dont have victories anymore. we used to have victories but we dont have them. when was the last time anybody saw us beating lets say china in a trade deal. they kill us. i beat china all the time. all the time. when did we beat japan at anything. they send their cars over by the millions and what do we do. when was the last time you saw a chevrolet in tokyo. it doesnt exist folks. they beat us all the time. when do we beat mexico at the border. theyre laughing at us at our stupidity. and now they are beating us economically. they are not our friend believe me. but theyre killing us economically. the u.s. has become a dumping ground for everybody elses problems.  thank you. its true and these are the best and the finest. when mexico sends its people theyre not sending their best. theyre not sending you. theyre not sending you. theyre sending people that have lots of problems and theyre bringing those problems with us. theyre bringing drugs. theyre bringing crime. theyre rapists. and some i assume are good people. but i speak to border guards and they tell us what were getting. and it only makes common sense. it only makes common sense. theyre sending us not the right people. its coming from more than mexico. its coming from all over south and latin america and its coming probably a probably a from the middle east. but we dont know. because we have no protection and we have no competence we dont know whats happening. and its got to stop and its got to stop fast. islamic terrorism is eating up large portions of the middle east. theyve become rich. im in competition with them. they just built a hotel in syria. can you believe this. they built a hotel. when i have to build a hotel i pay interest. they dont have to pay interest because they took the oil that when we left iraq i said we shouldve taken. so now isis has the oil and what they dont have iran has. and in  a and i will tell you this and i said it very strongly years ago i said a and i love the military and i want to have the strongest military that weve ever had and we need it more now than ever. but i said dont hit iraq because youre going to totally destabilize the middle east. iran is going to take over the middle east iran and somebody else will get the oil and it turned out that iran is now taking over iraq. think of it. iran is taking over iraq and theyre taking it over big league. we spent  trillion in iraq  trillion. we lost thousands of lives thousands in iraq. we have wounded soldiers who i love i love a theyre great a all over the place thousands and thousands of wounded soldiers. and we have nothing. we cant even go there. we have nothing. and every time we give iraq equipment the first time a bullet goes off in the air they leave it. last week i read  humvees a these are big vehicles a were left behind for the enemy. . you would say maybe two maybe four.  sophisticated vehicles they ran and the enemy took them. youre right. last quarter it was just announced our gross domestic product a a sign of strength right. but not for us. it was below zero. whoever heard of this. its never below zero. our labor participation rate was the worst since . but think of it gdp below zero horrible labor participation rate. and our real unemployment is anywhere from  to  percent. dont believe the .. dont believe it. thats right. a lot of people up there cant get jobs. they cant get jobs because there are no jobs because china has our jobs and mexico has our jobs. they all have jobs. but the real number the real number is anywhere from  to  and maybe even  percent and nobody talks about it because its a statistic thats full of nonsense. our enemies are getting stronger and stronger by the way and we as a country are getting weaker. even our nuclear arsenal doesnt work. it came out recently they have equipment that is  years old. they dont know if it worked. and i thought it was horrible when it was broadcast on television because boy does that send signals to putin and all of the other people that look at us and they say that is a group of people and that is a nation that truly has no clue. they dont know what theyre doing. they dont know what theyre doing. we have a disaster called the big lie obamacare. obamacare. yesterday it came out that costs are going for people up    and even  percent and deductibles are through the roof. you have to be hit by a tractor literally a tractor to use it because the deductibles are so high its virtually useless. its virtually useless. it is a disaster. and remember the  billion web site.  billion we spent on a web site and to this day it doesnt work. a  billion web site. i have so many web sites i have them all over the place. i hire people they do a web site. it costs me .  billion web site. well you need somebody because politicians are all talk no action. nothings gonna get done. they will not bring us a believe me a to the promised land. they will not. as an example ive been on the circuit making speeches and i hear my fellow republicans. and theyre wonderful people. i like them. they all want me to support them. they dont know how to bring it about. they come up to my office. im meeting with three of them in the next week. and they dont know a are you running. are you not running. could we have your support. what do we do. how do we do it. i like them. and i hear their speeches. and they dont talk jobs and they dont talk china. when was the last time you heard china is killing us. theyre devaluing their currency to a level that you wouldnt believe. it makes it impossible for our companies to compete impossible. theyre killing us. but you dont hear that from anybody else. you dont hear it from anybody else. and i watch the speeches. thank you. i watch the speeches of these people and they say the sun will rise the moon will set all sorts of wonderful things will happen. and people are saying whats going on. i just want a job. just get me a job. i dont need the rhetoric. i want a job. and thats whats happening. and its going to get worse because remember obamacare really kicks in in  . obama is going to be out playing golf. he might be on one of my courses. i would invite him i actually would say. i have the best courses in the world so id say you what if he wants to a i have one right next to the white house right on the potomac. if hed like to play thats fine.  in fact id love him to leave early and play that would be a very good thing.  but obamacare kicks in in . really big league. it is going to be amazingly destructive. doctors are quitting. i have a friend whos a doctor and he said to me the other day donald i never saw anything like it. i have more accountants than i have nurses. its a disaster. my patients are beside themselves. they had a plan that was good. they have no plan now. we have to repeal obamacare and it can be a and a and it can be replaced with something much better for everybody. let it be for everybody. but much better and much less expensive for people and for the government. and we can do it. so ive watched the politicians. ive dealt with them all my life. if you cant make a good deal with a politician then theres something wrong with you. youre certainly not very good. and thats what we have representing us. they will never make america great again. they dont even have a chance. theyre controlled fully a theyre controlled fully by the lobbyists by the donors and by the special interests fully. yes they control them. hey i have lobbyists. i have to tell you. i have lobbyists that can produce anything for me. theyre great. but you know what. it wont happen. it wont happen. because we have to stop doing things for some people but for this country its destroying our country. we have to stop and it has to stop now. now our country needs a our country needs a truly great leader and we need a truly great leader now. we need a leader that wrote the art of the deal. we need a leader that can bring back our jobs can bring back our manufacturing can bring back our military can take care of our vets. our vets have been abandoned.  and we also need a cheerleader. you know when president obama was elected i said well the one thing i think hell do well. i think hell be a great cheerleader for the country. i think hed be a great spirit. he was vibrant. he was young. i really thought that he would be a great cheerleader. hes not a leader. thats true. youre right about that. but he wasnt a cheerleader. hes actually a negative force. hes been a negative force. he wasnt a cheerleader he was the opposite. we need somebody that can take the brand of the united states and make it great again. its not great again.  we need a we need somebody a we need somebody that literally will take this country and make it great again. we can do that.  and i will tell you i love my life. i have a wonderful family. theyre saying dad youre going to do something thats going to be so tough. you know all of my life ive heard that a truly successful person a really really successful person and even modestly successful cannot run for public office. just cant happen. and yet thats the kind of mindset that you need to make this country great again. so ladies and gentlemen...  i am officially running...... for president of the united states and we are going to make our country great again.  it can happen. our country has tremendous potential. we have'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedNormalizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefTrain = 0.8\n",
    "nbTrain = int(len(cleanedNormalizedData)*coefTrain)\n",
    "trainData, testData = cleanedNormalizedData[:nbTrain], cleanedNormalizedData[nbTrain:]\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_trump(Dataset):\n",
    "    def __init__(self, data, target, length=10):\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "        self.size = len(data)-self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index:index+self.length], self.data[index+self.length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_trump(trainData, None), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_trump(testData, None), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(id2lettre), len(id2lettre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "latent_size = 10\n",
    "input_dim = len(id2lettre)\n",
    "output_dim = len(id2lettre)\n",
    "lr=1e-3\n",
    "\n",
    "model = GRU(latent_size, input_dim, output_dim)\n",
    "#model = nn.RNN(input_size=input_dim, hidden_size=latent_size, num_layers=1, nonlinearity=\"tanh\", batch_first=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Itérations 0: train loss 3.022400140762329, test loss 3.0349550247192383\n",
      "Itérations 1: train loss 2.995967149734497, test loss 3.29297137260437\n",
      "Itérations 2: train loss 2.661055564880371, test loss 2.4504425525665283\n",
      "Itérations 3: train loss 2.558166265487671, test loss 2.3273019790649414\n",
      "Itérations 4: train loss 2.7611265182495117, test loss 2.4234821796417236\n",
      "Itérations 5: train loss 2.5236456394195557, test loss 3.126222610473633\n",
      "Itérations 6: train loss 2.9989500045776367, test loss 3.215949058532715\n",
      "Itérations 7: train loss 2.861516237258911, test loss 2.676893949508667\n",
      "Itérations 8: train loss 2.6598596572875977, test loss 2.107161045074463\n",
      "Itérations 9: train loss 2.5127615928649902, test loss 2.2882063388824463\n",
      "Itérations 10: train loss 2.4998700618743896, test loss 2.5899362564086914\n",
      "Itérations 11: train loss 2.437195062637329, test loss 2.687497854232788\n",
      "Itérations 12: train loss 2.4316647052764893, test loss 2.5005481243133545\n",
      "Itérations 13: train loss 2.489182472229004, test loss 2.816814422607422\n",
      "Itérations 14: train loss 2.5168724060058594, test loss 2.4443979263305664\n",
      "Itérations 15: train loss 2.599395513534546, test loss 2.4724574089050293\n",
      "Itérations 16: train loss 2.5462543964385986, test loss 2.3747308254241943\n",
      "Itérations 17: train loss 2.434370994567871, test loss 3.144606351852417\n",
      "Itérations 18: train loss 2.6929941177368164, test loss 2.7473859786987305\n",
      "Itérations 19: train loss 2.63515305519104, test loss 3.0996322631835938\n",
      "Itérations 20: train loss 2.7035374641418457, test loss 2.5334885120391846\n",
      "Itérations 21: train loss 2.478217124938965, test loss 2.2980775833129883\n",
      "Itérations 22: train loss 2.1694486141204834, test loss 2.596367359161377\n",
      "Itérations 23: train loss 2.4529030323028564, test loss 2.5449883937835693\n",
      "Itérations 24: train loss 3.000187873840332, test loss 2.745020866394043\n",
      "Itérations 25: train loss 2.5453295707702637, test loss 2.3511404991149902\n",
      "Itérations 26: train loss 2.358358383178711, test loss 2.2887072563171387\n",
      "Itérations 27: train loss 2.6164629459381104, test loss 2.2470080852508545\n",
      "Itérations 28: train loss 2.560392141342163, test loss 2.693995475769043\n",
      "Itérations 29: train loss 2.4064881801605225, test loss 2.0710394382476807\n",
      "Itérations 30: train loss 2.7067806720733643, test loss 2.801875114440918\n",
      "Itérations 31: train loss 2.51155686378479, test loss 2.2069857120513916\n",
      "Itérations 32: train loss 2.4474844932556152, test loss 2.6032862663269043\n",
      "Itérations 33: train loss 2.44756817817688, test loss 2.8221118450164795\n",
      "Itérations 34: train loss 2.3578715324401855, test loss 2.282639980316162\n",
      "Itérations 35: train loss 2.3688549995422363, test loss 2.325917959213257\n",
      "Itérations 36: train loss 2.43219256401062, test loss 2.8044614791870117\n",
      "Itérations 37: train loss 2.181873321533203, test loss 2.3153631687164307\n",
      "Itérations 38: train loss 2.4932193756103516, test loss 2.324294328689575\n",
      "Itérations 39: train loss 2.240196466445923, test loss 2.4279518127441406\n",
      "Itérations 40: train loss 2.3210535049438477, test loss 2.3904902935028076\n",
      "Itérations 41: train loss 2.4806065559387207, test loss 2.494694471359253\n",
      "Itérations 42: train loss 2.4238932132720947, test loss 2.2842628955841064\n",
      "Itérations 43: train loss 2.3117592334747314, test loss 2.410280466079712\n",
      "Itérations 44: train loss 2.229346752166748, test loss 2.5747013092041016\n",
      "Itérations 45: train loss 2.260881185531616, test loss 1.9829071760177612\n",
      "Itérations 46: train loss 2.493795394897461, test loss 2.9682180881500244\n",
      "Itérations 47: train loss 2.4496774673461914, test loss 2.6539924144744873\n",
      "Itérations 48: train loss 2.264521598815918, test loss 2.7615673542022705\n",
      "Itérations 49: train loss 2.562974452972412, test loss 2.0660736560821533\n",
      "Itérations 50: train loss 2.4869420528411865, test loss 2.590365171432495\n",
      "Itérations 51: train loss 2.748291492462158, test loss 2.3787178993225098\n",
      "Itérations 52: train loss 2.1123545169830322, test loss 2.543365478515625\n",
      "Itérations 53: train loss 2.3659937381744385, test loss 2.625061273574829\n",
      "Itérations 54: train loss 2.464705467224121, test loss 2.513125419616699\n",
      "Itérations 55: train loss 2.4071805477142334, test loss 2.171593189239502\n",
      "Itérations 56: train loss 2.8443949222564697, test loss 2.2212588787078857\n",
      "Itérations 57: train loss 2.4226877689361572, test loss 3.17755126953125\n",
      "Itérations 58: train loss 2.686180353164673, test loss 1.9651694297790527\n",
      "Itérations 59: train loss 2.2784152030944824, test loss 2.0774550437927246\n",
      "Itérations 60: train loss 2.4403536319732666, test loss 2.192553758621216\n",
      "Itérations 61: train loss 2.6078262329101562, test loss 2.7569658756256104\n",
      "Itérations 62: train loss 2.3992702960968018, test loss 2.1105198860168457\n",
      "Itérations 63: train loss 2.1676530838012695, test loss 2.6762263774871826\n",
      "Itérations 64: train loss 2.135596990585327, test loss 2.0266189575195312\n",
      "Itérations 65: train loss 2.496920585632324, test loss 2.4241974353790283\n",
      "Itérations 66: train loss 2.7718183994293213, test loss 2.352677583694458\n",
      "Itérations 67: train loss 2.107639789581299, test loss 2.9926559925079346\n",
      "Itérations 68: train loss 2.2131571769714355, test loss 2.9945201873779297\n",
      "Itérations 69: train loss 2.3986902236938477, test loss 2.471381902694702\n",
      "Itérations 70: train loss 2.4673750400543213, test loss 2.627843141555786\n",
      "Itérations 71: train loss 2.2725894451141357, test loss 2.51645827293396\n",
      "Itérations 72: train loss 2.412386655807495, test loss 2.299041986465454\n",
      "Itérations 73: train loss 2.3093039989471436, test loss 2.849440813064575\n",
      "Itérations 74: train loss 2.746971845626831, test loss 2.17828369140625\n",
      "Itérations 75: train loss 2.130943536758423, test loss 2.41477370262146\n",
      "Itérations 76: train loss 2.4054129123687744, test loss 2.5076513290405273\n",
      "Itérations 77: train loss 2.3646750450134277, test loss 2.7389442920684814\n",
      "Itérations 78: train loss 2.363924980163574, test loss 1.9339946508407593\n",
      "Itérations 79: train loss 2.469599962234497, test loss 2.431894063949585\n",
      "Itérations 80: train loss 2.278764486312866, test loss 2.1915104389190674\n",
      "Itérations 81: train loss 2.244021415710449, test loss 2.820920705795288\n",
      "Itérations 82: train loss 2.222289800643921, test loss 2.7010364532470703\n",
      "Itérations 83: train loss 2.168057441711426, test loss 2.8820037841796875\n",
      "Itérations 84: train loss 2.4165196418762207, test loss 3.2073123455047607\n",
      "Itérations 85: train loss 1.9875240325927734, test loss 2.1909067630767822\n",
      "Itérations 86: train loss 2.1458611488342285, test loss 2.703693151473999\n",
      "Itérations 87: train loss 2.1507551670074463, test loss 3.146390199661255\n",
      "Itérations 88: train loss 2.3021433353424072, test loss 1.988540530204773\n",
      "Itérations 89: train loss 2.2473764419555664, test loss 2.554926633834839\n",
      "Itérations 90: train loss 2.4128191471099854, test loss 2.6563804149627686\n",
      "Itérations 91: train loss 2.2968108654022217, test loss 2.4124984741210938\n",
      "Itérations 92: train loss 2.3348426818847656, test loss 2.3559939861297607\n",
      "Itérations 93: train loss 2.450425624847412, test loss 2.6268866062164307\n",
      "Itérations 94: train loss 2.5465619564056396, test loss 2.444385290145874\n",
      "Itérations 95: train loss 2.164839029312134, test loss 2.7745988368988037\n",
      "Itérations 96: train loss 2.375123977661133, test loss 2.854346513748169\n",
      "Itérations 97: train loss 2.412043809890747, test loss 3.1017770767211914\n",
      "Itérations 98: train loss 2.3296380043029785, test loss 2.147214651107788\n",
      "Itérations 99: train loss 2.138941764831543, test loss 2.5264041423797607\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X = embedding(strs2code(sequences))\n",
    "        y = strs2code(labels).squeeze(1)\n",
    "\n",
    "        hidden_states = model(X.permute(1,0,2))\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "        train_loss = criterion(outputs, y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            X = embedding(strs2code(sequences))\n",
    "            y = strs2code(labels).squeeze(1)\n",
    "\n",
    "            hidden_states = model(X.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            test_loss = criterion(outputs, y)\n",
    "\n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération\n",
    "debut = \"thank y\"\n",
    "nbGenere = 20\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "xgens = []\n",
    "\n",
    "for i in range(nbGenere):\n",
    "    \n",
    "    if(i==0):#Première fois on forward la sequence\n",
    "        X = embedding(strs2code([debut]))\n",
    "        hidden_states = model(X.permute(1,0,2))\n",
    "        hgen = hidden_states[-1]\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    else:#Ensuite on génère en one step\n",
    "        x = embedding(strs2code([xgen])).squeeze(0)\n",
    "        hgen = model.one_step(x,hgen)\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    xgens.append(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o to to to to to to '"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xgens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
