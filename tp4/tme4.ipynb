{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fill_na(mat):\n",
    "    ix,iy = np.where(np.isnan(mat))\n",
    "    for i,j in zip(ix,iy):\n",
    "        if np.isnan(mat[i+1,j]):\n",
    "            mat[i,j]=mat[i-1,j]\n",
    "        else:\n",
    "            mat[i,j]=(mat[i-1,j]+mat[i+1,j])/2.\n",
    "    return mat\n",
    "\n",
    "\n",
    "def read_temps(path):\n",
    "    \"\"\"Lit le fichier de températures\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"rt\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if not row[1].replace(\".\",\"\").isdigit():\n",
    "                continue\n",
    "            data.append([float(x) if x != \"\" else float('nan') for x in row[1:]])\n",
    "    return torch.tensor(fill_na(np.array(data)), dtype=torch.float)\n",
    "\n",
    "\n",
    "\n",
    "class Dataset_temp(Dataset):\n",
    "    def __init__(self, data, target, lenght=50):\n",
    "        self.data = data\n",
    "        self.lenght = lenght\n",
    "        self.size = self.data.shape[0]-self.lenght+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.data[:,index], self.target[:,:,index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size*self.data.shape[1]\n",
    "\n",
    "class Dataset_temp2(Dataset):\n",
    "    def __init__(self, data, target):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col = index//self.size\n",
    "        lin = index%self.size\n",
    "        return (self.data[lin:lin+self.lenght, col], col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_encode = torch.tanh\n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearX = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.linearH = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.act_encode(self.linearX(x) + self.linearH(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = torch.zeros((length, batch, self.latent_size), dtype=torch.float)\n",
    "        res[0] = self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)) \n",
    "\n",
    "        for i in range(1,length):\n",
    "            res[i] = self.one_step(x[i], res[i-1].clone())\n",
    "\n",
    "        return res\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.empty(3, dtype=torch.long).random_(5)\n",
    "output = loss(input, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7621, -0.2009, -0.3928,  1.3664,  0.9750],\n",
       "        [-1.3933,  1.4714,  0.4377,  0.4443,  0.1470],\n",
       "        [ 0.7814, -0.5653, -1.0182, -2.1153, -0.0603]], requires_grad=True)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File exo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_temps, device, RNN, Dataset_temp\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#  TODO:  Question 2 : prédiction de la ville correspondant à une séquence\n",
    "\n",
    "temp_test, temp_test_labels = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(1), torch.arange(30)\n",
    "temp_train, temp_train_labels = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(1), torch.arange(30)\n",
    "print(f\"train shape {temp_train.shape}\")\n",
    "print(f\"test shape {temp_test.shape}\")\n",
    "\n",
    "import ipdb; ipdb.set_trace()\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "train_loader = DataLoader(Dataset_temp(temp_train, temp_train_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, temp_test_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "latent_size = 20\n",
    "input_dim = 1\n",
    "output_dim = temp_train.shape[1]\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[model.Wx,model.Wh,model.Wd,model.bh,model.bd],lr=1e-3)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "error = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states = model(sequences)\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "        train_loss = error(outputs, sequences)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(sequences)\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "        test_loss = error(outputs, sequences)\n",
    "        \n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_temp(Dataset):\n",
    "    def __init__(self, data, target, lenght=50):\n",
    "        self.data = data\n",
    "        self.lenght = lenght\n",
    "        self.size = self.data.shape[0]-self.lenght+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col = index//self.size\n",
    "        lin = index%self.size\n",
    "        return (self.data[lin:lin+self.lenght, col], col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size*self.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(2)\n",
    "temp_test = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = temp_train[:, :10]\n",
    "temp_test = temp_test[:, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "train_loader = DataLoader(Dataset_temp(temp_train, None, 50), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, None, 50), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "latent_size = 20\n",
    "input_dim = 1\n",
    "output_dim = 10 #number of class\n",
    "lr=1e-3\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "#model = RNN(input_dim, latent_size, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-240-073fea746435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-214-7230bef5d4d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mformat_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[0mstack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m     \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\traceback.py\u001b[0m in \u001b[0;36mextract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    360\u001b[0m                 filename, lineno, name, lookup_line=False, locals=f_locals))\n\u001b[0;32m    361\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m             \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;31m# If immediate lookup was desired, trigger lookups now.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlookup_lines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\compilerop.py\u001b[0m in \u001b[0;36mcheck_linecache_ipython\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \"\"\"\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# First call the original checkcache as intended\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m     \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkcache_ori\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m     \u001b[1;31m# Then, update back the cache with our data, so that tracebacks related\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;31m# to our compiled codes can be produced.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\linecache.py\u001b[0m in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mcontinue\u001b[0m   \u001b[1;31m# no-op for files loaded via a __loader__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m             \u001b[0mstat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            hidden_states = model(sequences.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            \n",
    "            train_loss = criterion(outputs, labels)\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "        model.eval()\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "\n",
    "                hidden_states = model(sequences.permute(1,0,2))\n",
    "                outputs = model.decode(hidden_states[-1])\n",
    "                test_loss = criterion(outputs, labels)\n",
    "\n",
    "            #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "      #if(epoch%10==0):\n",
    "        print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3, 1])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 7, 1])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.permute(1,0,2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 30])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6793,  0.1211, -0.6252, -0.1566,  0.7562, -0.6923, -0.7007,  0.0796,\n",
       "          0.0245,  0.7754,  0.2657,  0.0636, -0.8547, -0.7040,  0.6100, -0.3331,\n",
       "          0.0033,  0.1281, -0.4460, -0.5006,  0.8644,  0.5762, -0.2942, -0.4256,\n",
       "          0.3152,  0.4934, -0.0391, -0.6734,  0.0656,  0.6186],\n",
       "        [ 0.6793,  0.1211, -0.6252, -0.1566,  0.7562, -0.6923, -0.7007,  0.0796,\n",
       "          0.0245,  0.7754,  0.2657,  0.0636, -0.8547, -0.7040,  0.6100, -0.3331,\n",
       "          0.0033,  0.1281, -0.4460, -0.5006,  0.8644,  0.5762, -0.2942, -0.4256,\n",
       "          0.3152,  0.4934, -0.0391, -0.6734,  0.0656,  0.6186],\n",
       "        [ 0.6793,  0.1211, -0.6252, -0.1566,  0.7562, -0.6923, -0.7007,  0.0796,\n",
       "          0.0245,  0.7754,  0.2657,  0.0636, -0.8547, -0.7040,  0.6100, -0.3331,\n",
       "          0.0033,  0.1281, -0.4460, -0.5006,  0.8644,  0.5762, -0.2942, -0.4256,\n",
       "          0.3152,  0.4934, -0.0391, -0.6734,  0.0656,  0.6186]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1091],\n",
       "        [-0.1091],\n",
       "        [-0.1091]], grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "        [[ 0.7301, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.7301, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.7301, -1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "\n",
       "        [[ 0.5154, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5154, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "         [ 0.5153, -1.0000,  1.0000,  1.0000,  1.0000]]], grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5154, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5154, -1.0000,  1.0000,  1.0000,  1.0000],\n",
       "        [ 0.5153, -1.0000,  1.0000,  1.0000,  1.0000]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33342, 30, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33342, 1, 30])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_train, temp_train_labels = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(1), torch.arange(30)\n",
    "temp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_temps, device, RNN, Dataset_temp\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#  TODO:  Question 2 : prédiction de la ville correspondant à une séquence\n",
    "\n",
    "temp_test, temp_test_labels = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(1), torch.arange(30)\n",
    "temp_train, temp_train_labels = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(1), torch.arange(30)\n",
    "print(f\"train shape {temp_train.shape}\")\n",
    "print(f\"test shape {temp_test.shape}\")\n",
    "\n",
    "import ipdb; ipdb.set_trace()\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "train_loader = DataLoader(Dataset_temp(temp_train, temp_train_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, temp_test_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "latent_size = 20\n",
    "input_dim = 1\n",
    "output_dim = temp_train.shape[1]\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[model.Wx,model.Wh,model.Wd,model.bh,model.bd],lr=1e-3)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "error = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states = model(sequences)\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "        train_loss = error(outputs, sequences)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(sequences)\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "        test_loss = error(outputs, sequences)\n",
    "        \n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
