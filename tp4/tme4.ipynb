{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "1) Pour l'entrainement du RNN pour la génération (tp4), on utilise des séquences fixes. Qu'est-ce que votre dataset vous renvoie comme X et y? \n",
    "Car de mon coté, mon dataset va par exemple me donner X=\"thank yo\" et y=\"u\". Mais de doit-on pas plutot renvoyer X=\"thank you\" et pas de y puis entrainer le modèle en prenant:\n",
    "- X=\"t\" et y=\"h\" puis \n",
    "- X=\"th\" et y=\"a\" puis\n",
    "- X=\"tha\" et y=\"n\" etc ?\n",
    "\n",
    "on va par exemple me donner la séquence \"thank yo\" et la taget associé sera \"u\". C'est ce que vous faites? (pcq je crois qu'on peut pour une séquence \"thank you\" , on peut faire autant d'entrainement que de longueur de séquence genre on prend \"t\" pour prédire \"h\" puis \"th\" pour prédire \"a\" puis \"tha\" pour prédire \"n\" etc, jusqu'au botu de la séquence?)\n",
    "\n",
    "Vu le code du tp5, le loader peut ne renvoyer que un X (et pas un couple X,y) donc peut etre qu'il faut re,voyer une sequence et faire l'apprentissage sur toutes les sous-sequences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fill_na(mat):\n",
    "    ix,iy = np.where(np.isnan(mat))\n",
    "    for i,j in zip(ix,iy):\n",
    "        if np.isnan(mat[i+1,j]):\n",
    "            mat[i,j]=mat[i-1,j]\n",
    "        else:\n",
    "            mat[i,j]=(mat[i-1,j]+mat[i+1,j])/2.\n",
    "    return mat\n",
    "\n",
    "\n",
    "def read_temps(path):\n",
    "    \"\"\"Lit le fichier de températures\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"rt\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if not row[1].replace(\".\",\"\").isdigit():\n",
    "                continue\n",
    "            data.append([float(x) if x != \"\" else float('nan') for x in row[1:]])\n",
    "    return torch.tensor(fill_na(np.array(data)), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_encode = torch.tanh\n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearX = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.linearH = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.act_encode(self.linearX(x) + self.linearH(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearZ = nn.Linear(input_dim+latent_dim, latent_dim, bias=False)\n",
    "        self.linearR = nn.Linear(input_dim+latent_dim, input_dim+latent_dim, bias=False)\n",
    "        self.linearH = nn.Linear(input_dim+latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        zt = self.sigmoid(self.linearZ(concatHX))\n",
    "        rt = self.sigmoid(self.linearR(concatHX))\n",
    "        ht = (1-zt)*h + zt* self.tanh(self.linearH(rt*concatHX))\n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMv1(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "        \n",
    "        self.ct = torch.zeros((BATCH_SIZE, latent_dim))\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearF = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearI = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearC = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearO = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        ft = self.sigmoid(self.linearF(concatHX))\n",
    "        it = self.sigmoid(self.linearI(concatHX))\n",
    "        newCt = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        #self.ct = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        ot = self.sigmoid(self.linearO(concatHX))\n",
    "        ht = ot*self.tanh(newCt)\n",
    "        self.ct = newCt\n",
    "        \n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.sigmoid = torch.sigmoid\n",
    "        self.tanh = torch.tanh\n",
    "        \n",
    "        self.cts = [torch.zeros((BATCH_SIZE, latent_dim))]\n",
    "\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearF = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearI = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearC = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        self.linearO = nn.Linear(input_dim+latent_dim, latent_dim, bias=True)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        concatHX = torch.cat((x, h), 1)\n",
    "        ft = self.sigmoid(self.linearF(concatHX))\n",
    "        it = self.sigmoid(self.linearI(concatHX))\n",
    "        ct = ft*self.cts[-1] + it*self.tanh(self.linearC(concatHX))\n",
    "        #self.ct = ft*self.ct.clone() + it*self.tanh(self.linearC(concatHX))\n",
    "        ot = self.sigmoid(self.linearO(concatHX))\n",
    "        ht = ot*self.tanh(ct)\n",
    "        \n",
    "        self.cts.append(ct)\n",
    "        \n",
    "        return ht\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        #delete all cts\n",
    "        #self.cts = [self.cts[-1]]\n",
    "        \n",
    "        #forward\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.tanh(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_temp(Dataset):\n",
    "    def __init__(self, data, target, lenght=50):\n",
    "        self.data = data\n",
    "        self.lenght = lenght\n",
    "        self.size = self.data.shape[0]-self.lenght+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col = index//self.size\n",
    "        lin = index%self.size\n",
    "        return (self.data[lin:lin+self.lenght, col], col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size*self.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(2)\n",
    "temp_test = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasse = 5\n",
    "longueurData = 30\n",
    "BATCH_SIZE = 6\n",
    "longueurSeq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = temp_train[:longueurData, :nbClasse]\n",
    "temp_test = temp_test[:longueurData, :nbClasse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_temp(temp_train, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "latent_size = 10\n",
    "input_dim = 1\n",
    "output_dim = nbClasse\n",
    "lr=1e-3\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Itérations 0: train loss 1.9284852743148804, test loss 1.5937219858169556\n",
      "Itérations 1: train loss 1.7714735269546509, test loss 1.7347369194030762\n",
      "Itérations 2: train loss 1.7616195678710938, test loss 1.414864420890808\n",
      "Itérations 3: train loss 1.8163965940475464, test loss 1.6045713424682617\n",
      "Itérations 4: train loss 1.6932960748672485, test loss 1.6150871515274048\n",
      "Itérations 5: train loss 1.6077475547790527, test loss 1.4935122728347778\n",
      "Itérations 6: train loss 1.5200608968734741, test loss 1.5941352844238281\n",
      "Itérations 7: train loss 1.7681688070297241, test loss 1.613354206085205\n",
      "Itérations 8: train loss 1.571211338043213, test loss 1.8035459518432617\n",
      "Itérations 9: train loss 1.6167024374008179, test loss 1.371878743171692\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_states = model(sequences.permute(1,0,2))\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    \n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            hidden_states = model(sequences.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            test_loss = criterion(outputs, labels)\n",
    "\n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LETTRES = string.ascii_letters + string.punctuation+string.digits+' '\n",
    "#LETTRES = string.ascii_letters+' '\n",
    "LETTRES = string.ascii_letters[:26]+\".\"+' '\n",
    "id2lettre = dict(zip(range(1, len(LETTRES)+1), LETTRES))\n",
    "id2lettre[0] = ''\n",
    "lettre2id = dict(zip(id2lettre.values(), id2lettre.keys()))\n",
    "\n",
    "def normalize(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if c in LETTRES)\n",
    "\n",
    "def string2code(s):\n",
    "    return torch.tensor([lettre2id[c] for c in normalize(s)])\n",
    "\n",
    "def code2string(t):\n",
    "    if(type(t)!=list):\n",
    "        t = t.tolist()\n",
    "    return ''.join(id2lettre[i] for i in t)\n",
    "\n",
    "def str2code(s):\n",
    "    return [lettre2id[c] for c in s]\n",
    "\n",
    "def strs2code(ss):\n",
    "    return torch.LongTensor([str2code(s) for s in ss])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-traitement données trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTrumpData(s):\n",
    "    tmp = re.sub(\"\\[[^]]+\\]\", \"\", s) #delete non vocan words as [applause]\n",
    "    tmp = re.sub(\"[.?!]\", \".\", tmp)#replace end of phrase by .\n",
    "    tmp = re.sub(\":\\s*pmurT\\s*\\.\", \":%.\", tmp[::-1]) #reverse string and replace trump by %\n",
    "    tmp = re.sub(\":[^.%]+?\\.\", \":@.\", tmp) # place all no trump speaker by @\n",
    "    tmp = re.sub(\"^\\s*Trump\", \"%\", tmp[::-1]) #reverse string and replace first Trump by %\n",
    "    tmp = re.sub(\"@\\s*:[^%]+?%\", \"%\", tmp)  #delete words not say by trump\n",
    "    return re.sub(\"%:\", \"\", tmp)# delete %: wich is just to show wo speaks (but now it is trump every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/trump_full_speech.txt\", 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanedData = cleanTrumpData(data)\n",
    "cleanedData = cleanTrumpData(data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" wow. whoa. that is some group of people. thousands. so nice, thank you very much. that's really nice. thank you. it's great to be at trump tower. it's great to be in a wonderful city, new york. and it's an honor to have everybody here. this is beyond anybody's expectations. there's been no crowd like this. and, i can tell, some of the candidates, they went in. they didn't know the air-conditioner didn't work. they sweated like dogs.  they didn't know the room was too big, because they didn't have anybody there. how are they going to beat isis. i don't think it's gonna happen.  our country is in serious trouble. we don't have victories anymore. we used to have victories, but we don't have them. when was the last time anybody saw us beating, let's say, china in a trade deal. they kill us. i beat china all the time. all the time. when did we beat japan at anything. they send their cars over by the millions, and what do we do. when was the last time you saw a chevrolet in tokyo. it doesn'\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedData[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedNormalizedData = normalize(cleanedData)\n",
    "cleanedNormalizedData = cleanedNormalizedData[:1000]#To have a little sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' wow. whoa. that is some group of people. thousands. so nice thank you very much. thats really nice. thank you. its great to be at trump tower. its great to be in a wonderful city new york. and its an honor to have everybody here. this is beyond anybodys expectations. theres been no crowd like this. and i can tell some of the candidates they went in. they didnt know the airconditioner didnt work. they sweated like dogs.  they didnt know the room was too big because they didnt have anybody there. how are they going to beat isis. i dont think its gonna happen.  our country is in serious trouble. we dont have victories anymore. we used to have victories but we dont have them. when was the last time anybody saw us beating lets say china in a trade deal. they kill us. i beat china all the time. all the time. when did we beat japan at anything. they send their cars over by the millions and what do we do. when was the last time you saw a chevrolet in tokyo. it doesnt exist folks. they beat us'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanedNormalizedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefTrain = 0.8\n",
    "nbTrain = int(len(cleanedNormalizedData)*coefTrain)\n",
    "trainData, testData = cleanedNormalizedData[:nbTrain], cleanedNormalizedData[nbTrain:]\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_trump(Dataset):\n",
    "    def __init__(self, data, target, length=10):\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "        self.size = len(data)-self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index:index+self.length], self.data[index+self.length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_trump(trainData, None), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_trump(testData, None), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(id2lettre), len(id2lettre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "latent_size = 10\n",
    "input_dim = len(id2lettre)\n",
    "output_dim = len(id2lettre)\n",
    "lr=1e-3\n",
    "\n",
    "model = LSTM(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [39, 10]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-8f24caba6006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \"\"\"\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [39, 10]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            X = embedding(strs2code(sequences))\n",
    "            y = strs2code(labels).squeeze(1)\n",
    "\n",
    "            hidden_states = model(X.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "            train_loss = criterion(outputs, y)\n",
    "            train_loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "            #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "        model.eval()\n",
    "        for i, (sequences, labels) in enumerate(test_loader):\n",
    "            with torch.no_grad():\n",
    "                X = embedding(strs2code(sequences))\n",
    "                y = strs2code(labels).squeeze(1)\n",
    "\n",
    "                hidden_states = model(X.permute(1,0,2))\n",
    "                outputs = model.decode(hidden_states[-1])\n",
    "                test_loss = criterion(outputs, y)\n",
    "\n",
    "            #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "      #if(epoch%10==0):\n",
    "        print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération\n",
    "debut = \"thank y\"\n",
    "nbGenere = 20\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "xgens = []\n",
    "\n",
    "for i in range(nbGenere):\n",
    "    \n",
    "    if(i==0):#Première fois on forward la sequence\n",
    "        X = embedding(strs2code([debut]))\n",
    "        hidden_states = model(X.permute(1,0,2))\n",
    "        hgen = hidden_states[-1]\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    else:#Ensuite on génère en one step\n",
    "        x = embedding(strs2code([xgen])).squeeze(0)\n",
    "        hgen = model.one_step(x,hgen)\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    xgens.append(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ont thant thany tont'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(xgens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
