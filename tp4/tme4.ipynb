{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "def fill_na(mat):\n",
    "    ix,iy = np.where(np.isnan(mat))\n",
    "    for i,j in zip(ix,iy):\n",
    "        if np.isnan(mat[i+1,j]):\n",
    "            mat[i,j]=mat[i-1,j]\n",
    "        else:\n",
    "            mat[i,j]=(mat[i-1,j]+mat[i+1,j])/2.\n",
    "    return mat\n",
    "\n",
    "\n",
    "def read_temps(path):\n",
    "    \"\"\"Lit le fichier de températures\"\"\"\n",
    "    data = []\n",
    "    with open(path, \"rt\") as fp:\n",
    "        reader = csv.reader(fp, delimiter=',')\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            if not row[1].replace(\".\",\"\").isdigit():\n",
    "                continue\n",
    "            data.append([float(x) if x != \"\" else float('nan') for x in row[1:]])\n",
    "    return torch.tensor(fill_na(np.array(data)), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_encode = torch.tanh\n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearX = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.linearH = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.act_encode(self.linearX(x) + self.linearH(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = torch.zeros((length, batch, self.latent_size), dtype=torch.float)\n",
    "        res[0] = self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)) \n",
    "\n",
    "        for i in range(1,length):\n",
    "            res[i] = self.one_step(x[i], res[i-1].clone())\n",
    "\n",
    "        return res\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    #  TODO:  Implémenter comme décrit dans la question 1\n",
    "    def __init__(self, latent_dim, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.latent_size = latent_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.act_encode = torch.tanh\n",
    "        self.act_decode = torch.tanh\n",
    "\n",
    "        # Network parameters\n",
    "        self.linearX = nn.Linear(input_dim, latent_dim, bias=True)\n",
    "        self.linearH = nn.Linear(latent_dim, latent_dim, bias=False)\n",
    "        \n",
    "        self.linearD = nn.Linear(latent_dim, output_dim, bias=True)\n",
    "        \n",
    "\n",
    "    def one_step(self, x, h):\n",
    "        \"\"\" \n",
    "        compute the hidden state for one step of time\n",
    "        dim(x) = batch x dimX\n",
    "        dim(h) = batch x latent_size\n",
    "        \"\"\"\n",
    "        return self.act_encode(self.linearX(x) + self.linearH(h))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Treat a batch of sequences,\n",
    "        x -> batch of sequences, dim(X) = lenght_sequence x batch x dimX\n",
    "        h -> init hidden state, dim(h) = batch x latent_size\n",
    "\n",
    "        return a batch of hidden state sequences -> dim = lenght_sequence x batch x latent_size\n",
    "        \"\"\"\n",
    "        length, batch, dim = x.shape\n",
    "        res = []\n",
    "        res.append(self.one_step(x[0], torch.zeros((batch, self.latent_size), dtype=torch.float)))\n",
    "\n",
    "        for i in range(1,length):\n",
    "            res.append(self.one_step(x[i], res[i-1]))\n",
    "\n",
    "        return torch.stack(res)\n",
    "\n",
    "        \n",
    "    def decode(self, h):\n",
    "        \"\"\"\n",
    "        decode a batch of hidden state\n",
    "        \"\"\"\n",
    "        return self.act_decode(self.linearD(h))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File exo2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from utils import read_temps, device, RNN, Dataset_temp\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#  TODO:  Question 2 : prédiction de la ville correspondant à une séquence\n",
    "\n",
    "temp_test, temp_test_labels = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(1), torch.arange(30)\n",
    "temp_train, temp_train_labels = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(1), torch.arange(30)\n",
    "print(f\"train shape {temp_train.shape}\")\n",
    "print(f\"test shape {temp_test.shape}\")\n",
    "\n",
    "import ipdb; ipdb.set_trace()\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "train_loader = DataLoader(Dataset_temp(temp_train, temp_train_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, temp_test_labels), shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "num_epochs = 50\n",
    "latent_size = 20\n",
    "input_dim = 1\n",
    "output_dim = temp_train.shape[1]\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[model.Wx,model.Wh,model.Wd,model.bh,model.bd],lr=1e-3)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "error = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        hidden_states = model(sequences)\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "        train_loss = error(outputs, sequences)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            hidden_states = model(sequences)\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "        test_loss = error(outputs, sequences)\n",
    "        \n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_temp(Dataset):\n",
    "    def __init__(self, data, target, lenght=50):\n",
    "        self.data = data\n",
    "        self.lenght = lenght\n",
    "        self.size = self.data.shape[0]-self.lenght+1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        col = index//self.size\n",
    "        lin = index%self.size\n",
    "        return (self.data[lin:lin+self.lenght, col], col)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size*self.data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = read_temps(\"data/tempAMAL_train.csv\").unsqueeze(2)\n",
    "temp_test = read_temps(\"data/tempAMAL_test.csv\").unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbClasse = 5\n",
    "longueurData = 30\n",
    "BATCH_SIZE = 6\n",
    "longueurSeq = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = temp_train[:longueurData, :nbClasse]\n",
    "temp_test = temp_test[:longueurData, :nbClasse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_temp(temp_train, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_temp(temp_test, None, longueurSeq), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "latent_size = 10\n",
    "input_dim = 1\n",
    "output_dim = nbClasse\n",
    "lr=1e-3\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'permute'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-308-369a412a6e77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'permute'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        hidden_states = model(sequences.permute(1,0,2))\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "        train_loss = criterion(outputs, labels)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "\n",
    "            hidden_states = model(sequences.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            test_loss = criterion(outputs, labels)\n",
    "\n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TME Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code donnée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LETTRES = string.ascii_letters + string.punctuation+string.digits+' '\n",
    "#LETTRES = string.ascii_letters+' '\n",
    "LETTRES = string.ascii_letters[:26]+' '\n",
    "id2lettre = dict(zip(range(1, len(LETTRES)+1), LETTRES))\n",
    "id2lettre[0] = ''\n",
    "lettre2id = dict(zip(id2lettre.values(), id2lettre.keys()))\n",
    "\n",
    "def normalize(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if c in LETTRES)\n",
    "\n",
    "def string2code(s):\n",
    "    return torch.tensor([lettre2id[c] for c in normalize(s)])\n",
    "\n",
    "def code2string(t):\n",
    "    if(type(t)!=list):\n",
    "        t = t.tolist()\n",
    "    return ''.join(id2lettre[i] for i in t)\n",
    "\n",
    "def str2code(s):\n",
    "    return [lettre2id[c] for c in s]\n",
    "\n",
    "def strs2code(ss):\n",
    "    return torch.LongTensor([str2code(s) for s in ss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([20,  5, 19, 20, 53,  1, 22,  5,  3, 53,  1,  3,  3,  5, 14, 20, 53,  5,\n",
       "         3, 15, 12,  5])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2code(\"test avec accent école\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-traitement données trump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanTrumpData(s):\n",
    "    tmp = re.sub(\"\\[[^]]+\\]\", \"\", s) #delete non vocan words as [applause]\n",
    "    tmp = re.sub(\":\\s*pmurT\\s*\\.\", \":%.\", tmp[::-1]) #reverse string and replace trump by %\n",
    "    tmp = re.sub(\":[^.%]+?\\.\", \":@.\", tmp) # place all no trump speaker by @\n",
    "    tmp = re.sub(\"^\\s*Trump\", \"%\", tmp[::-1]) #reverse string and replace first Trump by %\n",
    "    tmp = re.sub(\"@\\s*:[^%]+?%\", \"%\", tmp)  #delete words not say by trump\n",
    "    return re.sub(\"%:\", \"\", tmp)# delete %: wich is just to show wo speaks (but now it is trump every time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/trump_full_speech.txt\", 'r') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanedData = cleanTrumpData(data)\n",
    "cleanedData = cleanTrumpData(data).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanedNormalizedData = normalize(cleanedData)\n",
    "cleanedNormalizedData = cleanedNormalizedData[:1000]#To have a little sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour bien séparer mais pas utile pour l'instant\n",
    "allData = cleanedNormalizedData.replace(\"!\",\".\").replace(\"?\",\".\")\n",
    "phrases = [phrase.strip() for phrase in allData.split(\".\")]\n",
    "phrases = [phrase for phrase in phrases if len(phrase)>0]\n",
    "histo = sorted(list(Counter([len(phrase) for phrase in phrases]).items()), reverse=False, key=lambda e:e[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefTrain = 0.8\n",
    "nbTrain = int(len(cleanedNormalizedData)*0.8)\n",
    "trainData, testData = cleanedNormalizedData[:nbTrain], cleanedNormalizedData[nbTrain:]\n",
    "BATCH_SIZE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_trump(Dataset):\n",
    "    def __init__(self, data, target, length=10):\n",
    "        self.data = data\n",
    "        self.length = length\n",
    "        self.size = len(data)-self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index:index+self.length], self.data[index+self.length]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(Dataset_trump(trainData, None), shuffle=True, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(Dataset_trump(testData, None), shuffle=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(id2lettre), len(id2lettre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "latent_size = 10\n",
    "input_dim = len(id2lettre)\n",
    "output_dim = len(id2lettre)\n",
    "lr=1e-3\n",
    "\n",
    "model = RNN(latent_size, input_dim, output_dim)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Itérations 0: train loss 2.8953452110290527, test loss 2.8922247886657715\n",
      "Itérations 1: train loss 3.145390272140503, test loss 3.3052873611450195\n",
      "Itérations 2: train loss 2.5619359016418457, test loss 2.4238903522491455\n",
      "Itérations 3: train loss 2.6971216201782227, test loss 3.0695183277130127\n",
      "Itérations 4: train loss 2.9537553787231445, test loss 2.2594547271728516\n",
      "Itérations 5: train loss 2.7399237155914307, test loss 2.204073905944824\n",
      "Itérations 6: train loss 2.2690963745117188, test loss 3.13906192779541\n",
      "Itérations 7: train loss 2.6233603954315186, test loss 3.176398754119873\n",
      "Itérations 8: train loss 3.2904183864593506, test loss 2.896482467651367\n",
      "Itérations 9: train loss 2.5344767570495605, test loss 3.260714054107666\n",
      "Itérations 10: train loss 2.139059543609619, test loss 2.4624903202056885\n",
      "Itérations 11: train loss 2.8210980892181396, test loss 2.468247652053833\n",
      "Itérations 12: train loss 2.9178080558776855, test loss 2.6756556034088135\n",
      "Itérations 13: train loss 2.341235399246216, test loss 3.222963333129883\n",
      "Itérations 14: train loss 2.4167795181274414, test loss 2.3263022899627686\n",
      "Itérations 15: train loss 2.1561195850372314, test loss 3.8175528049468994\n",
      "Itérations 16: train loss 2.3324201107025146, test loss 3.170445203781128\n",
      "Itérations 17: train loss 2.0973739624023438, test loss 2.449951171875\n",
      "Itérations 18: train loss 2.1897032260894775, test loss 3.030414581298828\n",
      "Itérations 19: train loss 2.405754327774048, test loss 2.6583571434020996\n",
      "Itérations 20: train loss 2.266254425048828, test loss 2.693598508834839\n",
      "Itérations 21: train loss 2.2584285736083984, test loss 2.0485424995422363\n",
      "Itérations 22: train loss 2.146693706512451, test loss 3.1408989429473877\n",
      "Itérations 23: train loss 2.9960272312164307, test loss 1.9650620222091675\n",
      "Itérations 24: train loss 3.1844403743743896, test loss 2.835014581680298\n",
      "Itérations 25: train loss 1.9520189762115479, test loss 2.000108242034912\n",
      "Itérations 26: train loss 2.9520468711853027, test loss 1.9637620449066162\n",
      "Itérations 27: train loss 1.8879694938659668, test loss 1.845751404762268\n",
      "Itérations 28: train loss 1.9665491580963135, test loss 2.399085521697998\n",
      "Itérations 29: train loss 2.050262928009033, test loss 1.8503038883209229\n",
      "Itérations 30: train loss 3.573836326599121, test loss 2.782522678375244\n",
      "Itérations 31: train loss 2.5014488697052, test loss 2.889249324798584\n",
      "Itérations 32: train loss 2.703249454498291, test loss 2.0182693004608154\n",
      "Itérations 33: train loss 1.7615259885787964, test loss 1.8660857677459717\n",
      "Itérations 34: train loss 1.8887004852294922, test loss 2.8129308223724365\n",
      "Itérations 35: train loss 2.0344882011413574, test loss 2.154693603515625\n",
      "Itérations 36: train loss 1.8502848148345947, test loss 2.7605345249176025\n",
      "Itérations 37: train loss 2.106414318084717, test loss 1.8525454998016357\n",
      "Itérations 38: train loss 2.04181170463562, test loss 2.2804906368255615\n",
      "Itérations 39: train loss 2.1853950023651123, test loss 2.9566500186920166\n",
      "Itérations 40: train loss 3.035663604736328, test loss 3.507385492324829\n",
      "Itérations 41: train loss 2.04317569732666, test loss 1.826155424118042\n",
      "Itérations 42: train loss 2.0032830238342285, test loss 2.7613208293914795\n",
      "Itérations 43: train loss 2.983269214630127, test loss 2.002063751220703\n",
      "Itérations 44: train loss 1.8685295581817627, test loss 2.1811752319335938\n",
      "Itérations 45: train loss 3.0989177227020264, test loss 2.8394551277160645\n",
      "Itérations 46: train loss 2.00944185256958, test loss 2.950011730194092\n",
      "Itérations 47: train loss 2.534067153930664, test loss 2.007967233657837\n",
      "Itérations 48: train loss 1.8847365379333496, test loss 1.776688814163208\n",
      "Itérations 49: train loss 1.9143846035003662, test loss 2.6168432235717773\n",
      "Itérations 50: train loss 2.056018590927124, test loss 1.9091715812683105\n",
      "Itérations 51: train loss 3.1250855922698975, test loss 2.955227851867676\n",
      "Itérations 52: train loss 2.0900416374206543, test loss 2.9943060874938965\n",
      "Itérations 53: train loss 1.932669997215271, test loss 3.031336784362793\n",
      "Itérations 54: train loss 3.423574924468994, test loss 2.6903023719787598\n",
      "Itérations 55: train loss 1.7422871589660645, test loss 2.346737861633301\n",
      "Itérations 56: train loss 1.784477710723877, test loss 2.8552746772766113\n",
      "Itérations 57: train loss 2.161970615386963, test loss 2.200309991836548\n",
      "Itérations 58: train loss 1.9457061290740967, test loss 2.260887622833252\n",
      "Itérations 59: train loss 2.1875686645507812, test loss 1.7916004657745361\n",
      "Itérations 60: train loss 2.0477452278137207, test loss 3.1228437423706055\n",
      "Itérations 61: train loss 1.6760706901550293, test loss 3.7578046321868896\n",
      "Itérations 62: train loss 2.938338279724121, test loss 3.1114373207092285\n",
      "Itérations 63: train loss 1.9185537099838257, test loss 1.7011303901672363\n",
      "Itérations 64: train loss 1.9297876358032227, test loss 1.8915804624557495\n",
      "Itérations 65: train loss 2.0512561798095703, test loss 2.988569974899292\n",
      "Itérations 66: train loss 2.1651859283447266, test loss 2.870905876159668\n",
      "Itérations 67: train loss 2.243814468383789, test loss 2.3343300819396973\n",
      "Itérations 68: train loss 2.009812355041504, test loss 3.879786491394043\n",
      "Itérations 69: train loss 1.8421919345855713, test loss 2.7292540073394775\n",
      "Itérations 70: train loss 1.971405267715454, test loss 2.039487361907959\n",
      "Itérations 71: train loss 2.0379176139831543, test loss 2.131930351257324\n",
      "Itérations 72: train loss 2.047177314758301, test loss 2.012221336364746\n",
      "Itérations 73: train loss 2.060628890991211, test loss 2.35705828666687\n",
      "Itérations 74: train loss 2.8241491317749023, test loss 2.5354223251342773\n",
      "Itérations 75: train loss 2.68265962600708, test loss 2.0242438316345215\n",
      "Itérations 76: train loss 2.791078567504883, test loss 3.179028034210205\n",
      "Itérations 77: train loss 2.451625347137451, test loss 2.2414212226867676\n",
      "Itérations 78: train loss 2.2824323177337646, test loss 1.7561542987823486\n",
      "Itérations 79: train loss 1.585850477218628, test loss 2.892889976501465\n",
      "Itérations 80: train loss 2.9634320735931396, test loss 3.0942201614379883\n",
      "Itérations 81: train loss 2.171210289001465, test loss 3.8336422443389893\n",
      "Itérations 82: train loss 1.8517061471939087, test loss 3.6589407920837402\n",
      "Itérations 83: train loss 2.9773457050323486, test loss 2.700035810470581\n",
      "Itérations 84: train loss 1.8738937377929688, test loss 2.8460898399353027\n",
      "Itérations 85: train loss 2.4589967727661133, test loss 2.6453025341033936\n",
      "Itérations 86: train loss 1.8293511867523193, test loss 1.939695119857788\n",
      "Itérations 87: train loss 1.9005073308944702, test loss 2.7215771675109863\n",
      "Itérations 88: train loss 2.7699217796325684, test loss 2.316765785217285\n",
      "Itérations 89: train loss 1.9172722101211548, test loss 3.099702835083008\n",
      "Itérations 90: train loss 2.087543249130249, test loss 2.8087358474731445\n",
      "Itérations 91: train loss 3.147831916809082, test loss 2.1082324981689453\n",
      "Itérations 92: train loss 2.1362829208374023, test loss 1.852196455001831\n",
      "Itérations 93: train loss 2.073486804962158, test loss 3.1149139404296875\n",
      "Itérations 94: train loss 1.9141802787780762, test loss 2.8991758823394775\n",
      "Itérations 95: train loss 2.099714756011963, test loss 1.86539626121521\n",
      "Itérations 96: train loss 1.8513293266296387, test loss 1.9481046199798584\n",
      "Itérations 97: train loss 1.9432729482650757, test loss 2.7386386394500732\n",
      "Itérations 98: train loss 2.943143129348755, test loss 2.7804582118988037\n",
      "Itérations 99: train loss 1.9719030857086182, test loss 3.158567190170288\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "print(\"Training ...\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X = embedding(strs2code(sequences))\n",
    "        y = strs2code(labels).squeeze(1)\n",
    "        \n",
    "        hidden_states = model(X.permute(1,0,2))\n",
    "        outputs = model.decode(hidden_states[-1])\n",
    "\n",
    "        train_loss = criterion(outputs, y)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    for i, (sequences, labels) in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            X = embedding(strs2code(sequences))\n",
    "            y = strs2code(labels).squeeze(1)\n",
    "            \n",
    "            hidden_states = model(X.permute(1,0,2))\n",
    "            outputs = model.decode(hidden_states[-1])\n",
    "            test_loss = criterion(outputs, y)\n",
    "\n",
    "        #writer.add_scalar('Loss/test', test_loss, epoch)\n",
    "  #if(epoch%10==0):\n",
    "    print(f\"Itérations {epoch}: train loss {train_loss}, test loss {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Génération\n",
    "debut = \"thank y\"\n",
    "nbGenere = 20\n",
    "sm = nn.Softmax(dim=1)\n",
    "\n",
    "xgens = []\n",
    "\n",
    "for i in range(nbGenere):\n",
    "    \n",
    "    if(i==0):#Première fois on forward la sequence\n",
    "        X = embedding(strs2code([debut]))\n",
    "        hidden_states = model(X.permute(1,0,2))\n",
    "        hgen = hidden_states[-1]\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    else:#Ensuite on génère en one step\n",
    "        x = embedding(strs2code([xgen])).squeeze(0)\n",
    "        hgen = model.one_step(x,hgen)\n",
    "        outputs = model.decode(hgen)\n",
    "        xgen = id2lettre[int(sm(outputs)[0].argmax())]\n",
    "    xgens.append(xgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['o', 'n', 't', ' ', 't', 'h', 'e', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ']"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
